{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "Name: Sravani Suravajhula\n",
    "UTA Id:1001778007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project i implemented Naive Bayes Classifier from scratch in python.\n",
    "for this assignment i used text dataset about movie review(http://ai.stanford.edu/~amaas/data/sentiment/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import copy\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict,Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(0)\n",
    "print_data=1\n",
    "\n",
    "train_neg=load_files('aclImdb/train', categories= ['neg'],encoding='utf-8')\n",
    "train_pos=load_files('aclImdb/train', categories= ['pos'],encoding='utf-8')\n",
    "test_neg=load_files('aclImdb/test', categories= ['neg'],encoding='utf-8')\n",
    "test_pos=load_files('aclImdb/test', categories= ['pos'],encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_data(data):\n",
    "    pattern1=re.compile(\"[!#$%&'()*+,\\'\\\"-./:;<=>?@[\\]^_`{|}~]\")\n",
    "    pattern2=re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    data=[re.sub(pattern1, '', line) for line in data]\n",
    "    data=[re.sub(pattern2, ' ', line).lower() for line in data]\n",
    "    return data\n",
    "    \n",
    "train_neg_data=cleanup_data(train_neg['data'])\n",
    "train_pos_data=cleanup_data(train_pos['data'])\n",
    "test_neg_data=cleanup_data(test_neg['data'])\n",
    "test_pos_data=cleanup_data(test_pos['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'theres', 'an', 'old', 'security', 'guard', 'and', 'a', 'guy', 'who', 'dies', 'and', 'then', 'theres', 'kevin', 'the', 'worlds', 'biggest', 'wuss', 'kevin', 'wants', 'to', 'impress', 'his', 'incredibly', 'insensitive', 'bratty', 'and', 'virginal', 'girlfriend', 'amy', 'as', 'he', 'returns', 'from', 'work', 'to', 'a', 'random', 'house', 'he', 'finds', 'his', 'friends', 'the', 'sexually', 'confusing', 'redshorted', 'kyle', 'and', 'the', 'truly', 'revolting', 'sluttish', 'daphne', 'they', 'are', 'soon', 'joined', 'by', 'daphnes', 'boyfriend', 'the', 'triggerhappy', 'sexcrazed', 'macho', 'lunkhead', 'nick', 'and', 'theres', 'the', 'title', 'creatures', 'horrid', 'little', 'dogeared', 'puppets', 'who', 'kill', 'people', 'by', 'giving', 'them', 'their', 'hearts', 'desire', 'kyles', 'hearts', 'desire', 'is', 'to', 'mate', 'with', 'a', 'creepy', 'yucky', 'woman', 'in', 'spandex', 'nicks', 'hearts', 'desire', 'is', 'to', 'throw', 'grenades', 'in', 'a', 'grade', 'school', 'cafeteria', 'i', 'mean', 'nightclub', 'kevins', 'hearts', 'desire', 'is', 'to', 'beat', 'up', 'a', 'skinny', 'thug', 'with', 'nunchucks', 'amys', 'hearts', 'desire', 'is', 'to', 'be', 'a', 'disgusting', 'slut', 'daphnes', 'already', 'a', 'disgusting', 'slut', 'so', 'she', 'doesnt', 'have', 'a', 'hearts', 'desire', 'along', 'the', 'way', 'a', 'truly', 'hideous', 'band', 'sings', 'a', 'truly', 'odd', 'song', 'the', 'hobgoblins', 'randomly', 'go', 'back', 'to', 'where', 'they', 'came', 'from', 'then', 'blow', 'up', 'citizen', 'kane', 'cannot', 'hold', 'a', 'candle', 'to', 'this', 'true', 'masterpiece', 'of', 'american', 'cinema']\n",
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "train_X=train_neg_data+train_pos_data\n",
    "train_Y=[0 for i in range(len(train_neg_data))]+[1 for i in range(len(train_pos_data))]\n",
    "\n",
    "test_X=test_neg_data+test_pos_data\n",
    "test_Y=[0 for i in range(len(test_neg_data))]+[1 for i in range(len(test_pos_data))]\n",
    "#print(train_X[0])\n",
    "train_X=[word.split() for word in train_X ]\n",
    "test_X=[word.split() for word in test_X ]\n",
    "\n",
    "print((train_X[0]))\n",
    "print(len(train_X),len(train_Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the dataset into train,development and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total words before omitting the uncommon words: 107625\n",
      "number of total words after ommiting the uncommon words: 28582\n"
     ]
    }
   ],
   "source": [
    "temp=random.randint(0,100)\n",
    "train_data_x,dev_data_x,train_data_y,dev_data_y=train_test_split(train_X,train_Y,test_size=0.2,random_state=temp)\n",
    "flatten = itertools.chain.from_iterable\n",
    "complete_data=list(flatten(train_data_x))\n",
    "complete_counter=Counter(complete_data)\n",
    "#print(complete_counter['the'])\n",
    "sorted_words=dict(sorted(complete_counter.items(),key=lambda i:i[1],reverse=True))\n",
    "print('number of total words before omitting the uncommon words:',len(sorted_words))\n",
    "for word in list(sorted_words):\n",
    "    if sorted_words[word] <5:\n",
    "        del sorted_words[word]\n",
    "print('number of total words after ommiting the uncommon words:',len(sorted_words))       \n",
    "required_words=list(sorted_words.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## counting number of words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'film', 'was', 'okay', 'quite', 'entertaining', 'the', 'cast', 'was', 'pretty', 'good', 'and', 'ill', 'second', 'what', 'the', 'comment', 'before', 'me', 'mentioned', 'glenn', 'quinn', 'was', 'outstanding', 'and', 'he', 'alone', 'is', 'reason', 'enough', 'to', 'watch', 'this', 'movie', 'he', 'played', 'the', 'selfish', 'evil', 'friend', 'and', 'manager', 'of', 'the', 'band', 'brilliantlybr', 'br', 'there', 'are', 'a', 'lot', 'of', 'songs', 'performed', 'by', 'beyond', 'gravity', 'in', 'this', 'film', 'but', 'this', 'doesnt', 'really', 'come', 'as', 'a', 'surprise', 'considering', 'the', 'film', 'is', 'a', 'vh1', 'production', 'however', 'if', 'the', 'soft', 'rock', 'pop', 'music', 'isnt', 'to', 'someones', 'liking', 'one', 'might', 'as', 'well', 'flash', 'forward', 'those', 'scenesbr', 'br', 'the', 'plot', 'of', 'a', 'band', 'trying', 'to', 'make', 'it', 'to', 'the', 'top', 'in', 'la', 'but', 'having', 'to', 'overcome', 'many', 'obstacles', 'on', 'the', 'way', 'isnt', 'too', 'original', 'but', 'quite', 'entertaining', 'with', 'some', 'surprising', 'plot', 'turns', 'here', 'and', 'there']\n",
      "{'the': 10, 'film': 3, 'was': 3, 'okay': 1, 'quite': 2, 'entertaining': 2, 'cast': 1, 'pretty': 1, 'good': 1, 'and': 4, 'ill': 1, 'second': 1, 'what': 1, 'comment': 1, 'before': 1, 'me': 1, 'mentioned': 1, 'glenn': 1, 'quinn': 1, 'outstanding': 1, 'he': 2, 'alone': 1, 'is': 2, 'reason': 1, 'enough': 1, 'to': 5, 'watch': 1, 'this': 3, 'movie': 1, 'played': 1, 'selfish': 1, 'evil': 1, 'friend': 1, 'manager': 1, 'of': 3, 'band': 2, 'brilliantlybr': 1, 'br': 2, 'there': 2, 'are': 1, 'a': 4, 'lot': 1, 'songs': 1, 'performed': 1, 'by': 1, 'beyond': 1, 'gravity': 1, 'in': 2, 'but': 3, 'doesnt': 1, 'really': 1, 'come': 1, 'as': 2, 'surprise': 1, 'considering': 1, 'vh1': 1, 'production': 1, 'however': 1, 'if': 1, 'soft': 1, 'rock': 1, 'pop': 1, 'music': 1, 'isnt': 2, 'someones': 1, 'liking': 1, 'one': 1, 'might': 1, 'well': 1, 'flash': 1, 'forward': 1, 'those': 1, 'scenesbr': 1, 'plot': 2, 'trying': 1, 'make': 1, 'it': 1, 'top': 1, 'la': 1, 'having': 1, 'overcome': 1, 'many': 1, 'obstacles': 1, 'on': 1, 'way': 1, 'too': 1, 'original': 1, 'with': 1, 'some': 1, 'surprising': 1, 'turns': 1, 'here': 1}\n"
     ]
    }
   ],
   "source": [
    "#print(train_data_x[0])\n",
    "\n",
    "train_data_x_cnted=[dict(Counter(word)) for word in train_data_x ]\n",
    "print(train_data_x_cnted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the probability of occurence of a word and\n",
    "## conditional probability based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive documents in training: 10042\n",
      "total negative documents in traing: 9958\n",
      "number of documents containing word \"the\": {'pos': 9946, 'neg': 9886}\n",
      "{'pos': 9946, 'neg': 9886, 'total': 19832, 'pos_prob': 0.9904401513642701, 'neg_prob': 0.9927696324563166, 'total_prob': 0.9916}\n"
     ]
    }
   ],
   "source": [
    "#print(train_data_x_cnted)\n",
    "total_words={}\n",
    "for i,line in enumerate(train_data_x_cnted):\n",
    "    #print(line)\n",
    "    if train_data_y[i]==1:\n",
    "        sent='pos'\n",
    "    else:\n",
    "        sent='neg'\n",
    "    for wrd in line:\n",
    "        \n",
    "        if not total_words.get(wrd) :\n",
    "            total_words[wrd]={'pos':0,'neg':0}\n",
    "        total_words[wrd][sent]+=1\n",
    "        \n",
    "pos_len=sum(train_data_y) \n",
    "print('total positive documents in training:',pos_len)\n",
    "neg_len=len(train_data_y)-pos_len\n",
    "print('total negative documents in traing:',neg_len)\n",
    "print('number of documents containing word \"the\":',total_words['the'])\n",
    "\n",
    "final_words={}\n",
    "for wrd in required_words:\n",
    "    final_words[wrd]=copy.deepcopy(total_words[wrd])\n",
    "    final_words[wrd]['total']=final_words[wrd]['pos']+final_words[wrd]['neg']\n",
    "    final_words[wrd]['pos_prob']=final_words[wrd]['pos']/pos_len\n",
    "    final_words[wrd]['neg_prob']=final_words[wrd]['neg']/neg_len\n",
    "    final_words[wrd]['total_prob']=final_words[wrd]['total']/(pos_len+neg_len)\n",
    "print(final_words['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the probability of occurence of a word and\n",
    "## conditional probability based on sentiment with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after smoothing: {'pos': 9946, 'neg': 9886, 'total': 19832, 'pos_prob': 0.9903424930306651, 'neg_prob': 0.9926706827309237, 'total_prob': 0.9915508449155085}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_words_smooth={}\n",
    "for wrd in required_words:\n",
    "    final_words_smooth[wrd]=copy.deepcopy(total_words[wrd])\n",
    "    final_words_smooth[wrd]['total']=final_words_smooth[wrd]['pos']+final_words_smooth[wrd]['neg']\n",
    "    final_words_smooth[wrd]['pos_prob']=(final_words_smooth[wrd]['pos']+1)/(pos_len+2)\n",
    "    final_words_smooth[wrd]['neg_prob']=(final_words_smooth[wrd]['neg']+1)/(neg_len+2)\n",
    "    final_words_smooth[wrd]['total_prob']=(final_words_smooth[wrd]['total']+1)/(pos_len+neg_len+2)\n",
    "print('after smoothing:',final_words_smooth['the'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy using Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Accuracy 75.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def nb_main(dev_x,dev_y):\n",
    "    dev_X_cnted=[list(Counter(word)) for word in dev_x ]\n",
    "    dev_y_pred=[]\n",
    "    verify_words=copy.deepcopy(final_words)\n",
    "    #print(dev_X_cnted[0])\n",
    "    accurate=0\n",
    "    for i,words in enumerate(dev_X_cnted):\n",
    "        pos_prob=1\n",
    "        neg_prob=1\n",
    "        for word in words:\n",
    "            if verify_words.get(word):\n",
    "                 if verify_words[word]['pos_prob']>0:\n",
    "                    pos_prob=pos_prob*verify_words[word]['pos_prob']\n",
    "            if verify_words.get(word):\n",
    "                if verify_words[word]['neg_prob']>0:\n",
    "                    neg_prob=neg_prob*verify_words[word]['neg_prob']    \n",
    "        if pos_prob>neg_prob:\n",
    "            dev_y_pred.append(1)\n",
    "        else:\n",
    "            dev_y_pred.append(0)\n",
    "        if dev_y[i]==dev_y_pred[i]:\n",
    "            accurate+=1\n",
    "\n",
    "    return dev_y_pred,accurate\n",
    "\n",
    "dev_y_pred,accurate=nb_main(dev_data_x,dev_data_y)\n",
    "accuracy=(accurate/len(dev_y_pred))*100\n",
    "print('Development Accuracy',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_main_smooth(dev_x,dev_y):\n",
    "    dev_X_cnted=[list(Counter(word)) for word in dev_x ]\n",
    "    dev_y_pred=[]\n",
    "    verify_words=copy.deepcopy(final_words_smooth)\n",
    "    #print(dev_X_cnted[0])\n",
    "    accurate=0\n",
    "    for i,words in enumerate(dev_X_cnted):\n",
    "        pos_prob=1\n",
    "        neg_prob=1\n",
    "        for word in words:\n",
    "            if verify_words.get(word):\n",
    "                 if verify_words[word]['pos_prob']>0:\n",
    "                    pos_prob=pos_prob*verify_words[word]['pos_prob']\n",
    "            if verify_words.get(word):\n",
    "                if verify_words[word]['pos_prob']>0:\n",
    "                    neg_prob=neg_prob*verify_words[word]['neg_prob']    \n",
    "        if pos_prob>neg_prob:\n",
    "            dev_y_pred.append(1)\n",
    "        else:\n",
    "            dev_y_pred.append(0)\n",
    "        if dev_y[i]==dev_y_pred[i]:\n",
    "            accurate+=1\n",
    "\n",
    "    return dev_y_pred,accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy using dev data by conducting five-fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dataset 1  is = 75.66 %\n",
      "Accuracy for dataset 2  is = 77.06 %\n",
      "Accuracy for dataset 3  is = 76.26 %\n",
      "Accuracy for dataset 4  is = 76.9 %\n",
      "Accuracy for dataset 5  is = 76.62 %\n",
      "five-fold dev data accuracy  without smoothing = 76.5 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dev_x=[]\n",
    "dev_y=[]\n",
    "for i in range(5):\n",
    "        temp=random.randint(0,100) \n",
    "        temp_train_x,temp_dev_x,temp_train_y,temp_dev_y=train_test_split(train_X,train_Y,test_size=0.2,random_state=temp)\n",
    "       \n",
    "        dev_x.append(temp_dev_x)\n",
    "        dev_y.append(temp_dev_y)\n",
    " \n",
    "\n",
    "def cross_validation(dev_list_x,dev_list_y,smooth):\n",
    "    k=5\n",
    "    accuracy=[]\n",
    "\n",
    "    for i in range(5):\n",
    "        temp_dev_x=dev_list_x[i]\n",
    "        temp_dev_y=dev_list_y[i]\n",
    "        if not smooth:\n",
    "            dev_y_pred,temp_correct=nb_main(temp_dev_x,temp_dev_y)\n",
    "        else:\n",
    "            dev_y_pred,temp_correct=nb_main_smooth(temp_dev_x,temp_dev_y)\n",
    "        temp_accuracy=(temp_correct/len(temp_dev_y))*100\n",
    "        \n",
    "        accuracy.append(temp_accuracy)\n",
    "        \n",
    "        print('Accuracy for dataset',i+1,' is =',round(temp_accuracy,2),'%')\n",
    "    return accuracy\n",
    "\n",
    "devdata_accuracy=cross_validation(dev_x,dev_y,False)\n",
    "print('five-fold dev data accuracy  without smoothing =',round(sum(devdata_accuracy)/len(devdata_accuracy),2),'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating five fold Dev data accuracy with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dataset 1  is = 83.18 %\n",
      "Accuracy for dataset 2  is = 83.92 %\n",
      "Accuracy for dataset 3  is = 83.3 %\n",
      "Accuracy for dataset 4  is = 84.26 %\n",
      "Accuracy for dataset 5  is = 83.94 %\n",
      "five-fold dev data accuracy with smoothing = 83.72 %\n"
     ]
    }
   ],
   "source": [
    "devdata_accuracy_smooth=cross_validation(dev_x,dev_y,True)\n",
    "print('five-fold dev data accuracy with smoothing =',round(sum(devdata_accuracy_smooth)/len(devdata_accuracy_smooth),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without smoothing we are ignoring conditinal probability for many words where as with smoothing we are considering all the words in the vocabulary thus we get true conditional probability with more accurate results.\n",
    "In my experiment this shows clearly as the accuracy of development dataset with five fold cross validation without smoothing is 76.5% With smoothing the accuracy is 83.72%.so, the accuracy improved with smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top 10 words that predicts positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Positive words: ['one', 'film', 'time', 'great', 'story', 'see', 'well', 'also', 'first', 'best']\n",
      "Top 10 Negative words: ['movie', 'like', 'even', 'good', 'would', 'bad', 'really', 'dont', 'much', 'get']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#top_pos_pred=dict(sorted(final_words.items(),key=lambda i:i[1]['pos_prob'],reverse=True))\n",
    "#top_neg_pred=dict(sorted(final_words.items(),key=lambda i:i[1]['neg_prob'],reverse=True))\n",
    "\n",
    "neg_pred={}\n",
    "pos_pred={}\n",
    "stop = set(stopwords.words('english'))\n",
    "actual_words=[wrd for wrd in final_words.keys() if not wrd in stop]\n",
    "actual_words=[wrd for wrd in actual_words if wrd.isalpha()]\n",
    "actual_words=[wrd for wrd in actual_words if len(wrd)>2]\n",
    "for dic in actual_words:\n",
    "\n",
    "    \n",
    "    if final_words[dic]['pos_prob']>final_words[dic]['neg_prob']:\n",
    "        pos_pred[dic]={'pos_prob':final_words[dic]['pos_prob']}\n",
    "    if final_words[dic]['neg_prob']>final_words[dic]['pos_prob']:\n",
    "        neg_pred[dic]={'neg_prob':final_words[dic]['neg_prob']}\n",
    "    \n",
    "top_pos_pred=dict(sorted(pos_pred.items() ,key=lambda i:i[1]['pos_prob'],reverse=True))\n",
    "#print(top_pos_pred)\n",
    "print('Top 10 Positive words:',list(top_pos_pred.keys())[:10])\n",
    "\n",
    "top_neg_pred=dict(sorted(neg_pred.items() ,key=lambda i:i[1]['neg_prob'],reverse=True))\n",
    "#print(top_pos_pred)\n",
    "print('Top 10 Negative words:',list(top_neg_pred.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using test data set calculating final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy without Smoothing is: 77.388 %\n"
     ]
    }
   ],
   "source": [
    "test_y_pred,test_correct=nb_main(test_X,test_Y)\n",
    "test_accuracy=(test_correct/len(test_y_pred))*100\n",
    "\n",
    "print('Test Accuracy without Smoothing is:',test_accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using test data set for calculating final accuracy with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with smoothing is: 79.908 %\n"
     ]
    }
   ],
   "source": [
    "test_y_pred,test_correct=nb_main_smooth(test_X,test_Y)\n",
    "test_accuracy=(test_correct/len(test_y_pred))*100\n",
    "\n",
    "print('Test Accuracy with smoothing is:',test_accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using five-fold cross validation for final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dataset 1  is = 77.1 %\n",
      "Accuracy for dataset 2  is = 78.3 %\n",
      "Accuracy for dataset 3  is = 77.28 %\n",
      "Accuracy for dataset 4  is = 77.78 %\n",
      "Accuracy for dataset 5  is = 76.94 %\n",
      "five-fold test accuracy without smoothing = 77.48 %\n"
     ]
    }
   ],
   "source": [
    "test_five_x=[]\n",
    "test_five_y=[]\n",
    "for i in range(5):\n",
    "        temp=random.randint(0,100) \n",
    "        _,temp_test_x,_,temp_test_y=train_test_split(test_X,test_Y,test_size=0.2,random_state=temp)\n",
    "       \n",
    "        test_five_x.append(temp_test_x)\n",
    "        test_five_y.append(temp_test_y)\n",
    " \n",
    "\n",
    "final_accuracy=cross_validation(test_five_x,test_five_y,False)\n",
    "print('five-fold test accuracy without smoothing =',round(sum(final_accuracy)/len(final_accuracy),2),'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## five-fold cross validation for final accuracy by using test data set with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dataset 1  is = 79.72 %\n",
      "Accuracy for dataset 2  is = 80.52 %\n",
      "Accuracy for dataset 3  is = 79.56 %\n",
      "Accuracy for dataset 4  is = 79.62 %\n",
      "Accuracy for dataset 5  is = 79.26 %\n",
      "five-fold test accuracy with smoothing = 79.74 %\n"
     ]
    }
   ],
   "source": [
    "final_accuracy_smooth=cross_validation(test_five_x,test_five_y,True)\n",
    "print('five-fold test accuracy with smoothing =',round(sum(final_accuracy_smooth)/len(final_accuracy_smooth),2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
